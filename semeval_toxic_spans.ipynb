{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semeval_toxic_spans.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPo2KreWIox9cPGIHPPixc4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morozowdmitry/semeval21/blob/master/semeval_toxic_spans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKpzPt3t23IX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import pickle\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from tqdm import tqdm\n",
        "\n",
        "!git clone https://github.com/ipavlopoulos/toxic_spans.git\n",
        "from toxic_spans.evaluation.semeval2021 import f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6C4ZiFZLWav"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHkmg-dCQMjx"
      },
      "source": [
        "#Выделяем токены, для токенов размечаем, токсичные они или нет"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tc2T6ca4BGH"
      },
      "source": [
        "df = pd.read_csv('/content/tsd_trial.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE_Kk3e-DqRq"
      },
      "source": [
        "texts = df['text']\n",
        "spans = df['spans']\n",
        "\n",
        "for i in range(df.shape[0]):\n",
        "  spans[i] = str(spans[i])\n",
        "  if spans[i]!='[]':\n",
        "    spans[i] = spans[i].replace('[','').replace(']','').replace('\\'','').replace('\"','').replace('\\\\','')\n",
        "    spans[i] = spans[i].split(', ')\n",
        "    spans[i] = [int(sp) for sp in spans[i]]\n",
        "  else:\n",
        "    spans[i] = []\n",
        "\n",
        "print(spans[0])\n",
        "print(type(spans[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC65NKkSBFjq"
      },
      "source": [
        "values = []\n",
        "values.append(['spans', 'text', 'tokens', 'is_toxic'])\n",
        "for i, text in enumerate(texts):\n",
        "  if i%1000 == 0:\n",
        "    print(i)\n",
        "  binstr = []\n",
        "  tokens = []\n",
        "  tmp=''\n",
        "  for j in range(len(text)):\n",
        "    if j==0:\n",
        "      tmp+=text[j]\n",
        "    else:\n",
        "      if j in spans[i] and j-1 not in spans[i]:\n",
        "        words = word_tokenize(tmp)\n",
        "        for z in range(len(words)):\n",
        "          tokens.append(words[z])\n",
        "          binstr.append(0)\n",
        "        tmp = text[j]\n",
        "      elif j not in spans[i] and j-1 in spans[i]:\n",
        "        words = word_tokenize(tmp)\n",
        "        for z in range(len(words)):\n",
        "          tokens.append(words[z])\n",
        "          binstr.append(1)\n",
        "        tmp = text[j]\n",
        "      else:\n",
        "        tmp+=text[j]\n",
        "  if tmp!='':\n",
        "    if len(text)-1 in spans[i]:\n",
        "      words = word_tokenize(tmp)\n",
        "      for z in range(len(words)):\n",
        "        tokens.append(words[z])\n",
        "        binstr.append(1)\n",
        "    else:\n",
        "      words = word_tokenize(tmp)\n",
        "      for z in range(len(words)):\n",
        "        tokens.append(words[z])\n",
        "        binstr.append(0)\n",
        "  #print(text)\n",
        "  #print(spans[i])\n",
        "  #print(binstr)\n",
        "  #print(tokens)\n",
        "  #print('******')\n",
        "  values.append([spans[i], text, tokens, binstr])\n",
        "\n",
        "pd.DataFrame(values).to_csv('tsd_trial_tokens.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR2cZLgGQX7m"
      },
      "source": [
        "#Делаем обучающую выборку для токенов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk5YJvCXQWYw"
      },
      "source": [
        "df = pd.read_csv('/content/tsd_trial_tokens.csv', index_col=0, header=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtZMaHn-RKPB"
      },
      "source": [
        "spans = df['spans']\n",
        "texts = df['text']\n",
        "tokens = df['tokens']\n",
        "is_toxic = df['is_toxic']\n",
        "\n",
        "for i in range(1, df.shape[0]):\n",
        "  spans[i] = str(spans[i])\n",
        "  if spans[i]!='[]':\n",
        "    spans[i] = spans[i].replace('[','').replace(']','').replace('\\'','').replace('\"','').replace('\\\\','')\n",
        "    spans[i] = spans[i].split(', ')\n",
        "    spans[i] = [int(sp) for sp in spans[i]]\n",
        "  else:\n",
        "    spans[i] = []\n",
        "\n",
        "for i in range(1, df.shape[0]):\n",
        "  tokens[i] = str(tokens[i])\n",
        "  if tokens[i]!='[]':\n",
        "    tokens[i] = tokens[i].replace('[','').replace(']','').replace('\\'','').replace('\"','').replace('\\\\','')\n",
        "    tokens[i] = tokens[i].split(', ')\n",
        "    #tokens[i] = [int(sp) for sp in tokens[i]]\n",
        "  else:\n",
        "    tokens[i] = []\n",
        "\n",
        "for i in range(1, df.shape[0]):\n",
        "  is_toxic[i] = str(is_toxic[i])\n",
        "  if is_toxic[i]!='[]':\n",
        "    is_toxic[i] = is_toxic[i].replace('[','').replace(']','').replace('\\'','').replace('\"','').replace('\\\\','')\n",
        "    is_toxic[i] = is_toxic[i].split(', ')\n",
        "    is_toxic[i] = [int(sp) for sp in is_toxic[i]]\n",
        "  else:\n",
        "    is_toxic[i] = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUW1n3y-Fw0Z"
      },
      "source": [
        "train_sample = []\n",
        "#train_sample.append(['is_toxic', 'token', 'token_lemmatized', 'text_id', 'text'])\n",
        "\n",
        "num = 0\n",
        "\n",
        "for i in range(1, spans.shape[0]):\n",
        "  if i%1000==0:\n",
        "    print(i)\n",
        "  for j, token in enumerate(tokens[i]):\n",
        "    if len(token)>=3:\n",
        "      train_sample.append([is_toxic[i][j], token, wordnet_lemmatizer.lemmatize(token), i, texts[i]])\n",
        "\n",
        "train_sample = pd.DataFrame(train_sample).sample(frac=1).values\n",
        "\n",
        "train_sample_short = []\n",
        "train_sample_short.append(['is_toxic', 'token', 'token_lemmatized', 'text_id', 'text'])\n",
        "\n",
        "\n",
        "for tr in train_sample:\n",
        "  if tr[0]==1:\n",
        "    train_sample_short.append(tr)\n",
        "  else:\n",
        "    if num<=20856:\n",
        "      train_sample_short.append(tr)\n",
        "      num+=1\n",
        "    \n",
        "pd.DataFrame(train_sample_short).to_csv('tsd_train_fnn.csv')\n",
        "print(len(train_sample_short))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3oaPRwDBYKV"
      },
      "source": [
        "train_sample = []\n",
        "train_sample.append(['is_toxic', 'token', 'token_lemmatized', 'text_id', 'text'])\n",
        "\n",
        "num = 0\n",
        "\n",
        "for i in range(1, spans.shape[0]):\n",
        "  if i%1000==0:\n",
        "    print(i)\n",
        "  for j, token in enumerate(tokens[i]):\n",
        "    if len(token)>=3:\n",
        "      train_sample.append([is_toxic[i][j], token, wordnet_lemmatizer.lemmatize(token), i, texts[i]])\n",
        "    \n",
        "pd.DataFrame(train_sample).to_csv('tsd_trial_fnn.csv')\n",
        "print(len(train_sample))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nw0V1FMfsav"
      },
      "source": [
        "#Получаем эмбеддинги BERT для текстов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oie8Z5NtCAXw"
      },
      "source": [
        "df = pd.read_csv('/content/tsd_trial.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwtcX0MqDAZr"
      },
      "source": [
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLL6J36tKmkc"
      },
      "source": [
        "sentences = df['text'].values\n",
        "sentence_embeddings = model.encode(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPcsMEbSKvSv"
      },
      "source": [
        "print(sentences[0])\n",
        "print(len(sentence_embeddings[0]))\n",
        "print(sentence_embeddings[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chGfTSrnK9g3"
      },
      "source": [
        "with open('/content/drive/My Drive/semeval21/sentence_embs_trial.pickle', 'wb') as f:\n",
        "  pickle.dump(sentence_embeddings, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52ulqbFCf0o3"
      },
      "source": [
        "#Получаем эмбеддинги для слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u50ddtlR5iO"
      },
      "source": [
        "df = pd.read_csv('/content/tsd_train_fnn.csv', index_col=0, header=1)\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "words = set()\n",
        "for el in df['token_lemmatized'].values:\n",
        "  words.add(wordnet_lemmatizer.lemmatize(el.lower().replace('.','').replace(' ','')))\n",
        "\n",
        "print(len(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyi9k3KwSIkW"
      },
      "source": [
        "df = pd.read_csv('/content/tsd_trial_fnn.csv', index_col=0, header=1)\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "for el in df['token_lemmatized'].values:\n",
        "  words.add(wordnet_lemmatizer.lemmatize(el.lower().replace('.','').replace(' ','')))\n",
        "\n",
        "print(len(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnmUtTjNhjUi"
      },
      "source": [
        "fname = '/content/drive/My Drive/english_w2v/model.bin'\n",
        "w2v = KeyedVectors.load_word2vec_format(fname, binary=True)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC3WIoErh7ZZ"
      },
      "source": [
        "words = list(words)\n",
        "print(words[:5])\n",
        "\n",
        "embs = []\n",
        "new_words = []\n",
        "\n",
        "for w in words:\n",
        "  try:\n",
        "    embs.append(w2v[w])\n",
        "    new_words.append(w)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "print(len(embs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dav2x2HlkH8"
      },
      "source": [
        "with open('/content/drive/My Drive/semeval21/word2vec_words.pickle', 'wb') as f:\n",
        "  pickle.dump(new_words, f)\n",
        "with open('/content/drive/My Drive/semeval21/word2vec_embs.pickle', 'wb') as f:\n",
        "  pickle.dump(embs, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZSm1nvOmY3J"
      },
      "source": [
        "#Делаем обучающую выборку: эмбеддинг слова + эмбеддинг предложения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj0mbdvbmc-z"
      },
      "source": [
        "df = pd.read_csv('/content/tsd_trial_fnn.csv', index_col=0, header =1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNnyg8PWNsYh"
      },
      "source": [
        "list_words =[]\n",
        "for el in df['token_lemmatized'].values:\n",
        "  list_words.append(wordnet_lemmatizer.lemmatize(el.lower().replace('.','').replace(' ','')) )\n",
        "\n",
        "print(len(list_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oggPOj95PPcJ"
      },
      "source": [
        "with open('/content/drive/My Drive/semeval21/word2vec_words.pickle', 'rb') as f:\n",
        "  words = pickle.load(f)\n",
        "with open('/content/drive/My Drive/semeval21/word2vec_embs.pickle', 'rb') as f:\n",
        "  embs = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/My Drive/semeval21/sentence_embs_trial.pickle', 'rb') as f:\n",
        "  sentence_embs = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hPMTgqwpYjS"
      },
      "source": [
        "train_data = []\n",
        "train_labels = []\n",
        "\n",
        "texts_words = []\n",
        "\n",
        "words_set = set(words)\n",
        "\n",
        "for i,el in enumerate(df.values):\n",
        "  if list_words[i] in words_set:\n",
        "    elem = np.concatenate((embs[words.index(list_words[i])], sentence_embs[el[3]-1]))\n",
        "    train_labels.append(el[0])\n",
        "    train_data.append(elem)\n",
        "    texts_words.append([el[1],el[4]])\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(train_labels))\n",
        "print(train_data[0])\n",
        "print(train_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8RLomkFUUtS"
      },
      "source": [
        "with open('/content/drive/My Drive/semeval21/test_data.pickle', 'wb') as f:\n",
        "  pickle.dump(train_data, f)\n",
        "with open('/content/drive/My Drive/semeval21/test_labels.pickle', 'wb') as f:\n",
        "  pickle.dump(train_labels, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xObzgJ79XEFl"
      },
      "source": [
        "with open('/content/drive/My Drive/semeval21/train_key.pickle', 'wb') as f:\n",
        "  pickle.dump(texts_words, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiDl9_z1XdHm"
      },
      "source": [
        "#Классификатор"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fsvvGnNXtai"
      },
      "source": [
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense, concatenate\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5QYFrc4X8j2"
      },
      "source": [
        "with open('/content/drive/My Drive/semeval21/test_data.pickle', 'rb') as f:\n",
        "  test_data = pickle.load(f)\n",
        "with open('/content/drive/My Drive/semeval21/test_labels.pickle', 'rb') as f:\n",
        "  test_labels = pickle.load(f)\n",
        "with open('/content/drive/My Drive/semeval21/train_data.pickle', 'rb') as f:\n",
        "  train_data = pickle.load(f)\n",
        "with open('/content/drive/My Drive/semeval21/train_labels.pickle', 'rb') as f:\n",
        "  train_labels = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRb5sbFCX8g9"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(train_labels))\n",
        "print(len(test_data))\n",
        "print(len(test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BItV3Y3h-60y"
      },
      "source": [
        "#labels to categorical\n",
        "\n",
        "train_labels = keras.utils.to_categorical(np.array(train_labels),2)\n",
        "test_labels = keras.utils.to_categorical(np.array(test_labels),2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmhUHUog_IOG"
      },
      "source": [
        "inputs=Input(shape=(len(train_data[0]),), name='input')\n",
        "x=Dense(1024, activation='tanh', name='fully_connected_1024_tanh')(inputs)\n",
        "#x=Dense(1024, activation='tanh', name='fully_connected_32')(x)\n",
        "predictions=Dense(2, activation='softmax', name='output_softmax')(x)\n",
        "model=Model(inputs=inputs, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugx3Uc-s_aK3"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "history = model.fit(np.array(train_data), train_labels, epochs=1, verbose=2,\\\n",
        "                    validation_data=(np.array(test_data), test_labels))\n",
        "predict = np.argmax(model.predict(np.array(test_data)), axis=1)\n",
        "answer = np.argmax(test_labels, axis=1)\n",
        "\n",
        "f1=f1_score(predict, answer)*100\n",
        "prec=precision_score(predict, answer)*100\n",
        "recall=recall_score(predict, answer)*100\n",
        "accuracy=accuracy_score(predict, answer)*100\n",
        "\n",
        "print(i)\n",
        "print('Готово!')\n",
        "print('f1 = {}, accuracy = {}, precision = {}, recall = {}'.format(f1,accuracy,prec,recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R46re1WKBuMZ"
      },
      "source": [
        "with open('/content/drive/My Drive/semeval21/test_key.pickle', 'rb') as f:\n",
        "  test_key = pickle.load(f)\n",
        "print(test_key[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgH2UFnJD387"
      },
      "source": [
        "df = pd.read_csv('/content/tsd_trial.csv')\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhb4jyxcDZ6j"
      },
      "source": [
        "final_spans = []\n",
        "for text in df['text'].values:\n",
        "  spans = []\n",
        "  for i, elem in enumerate(test_key):\n",
        "    if elem[1]==text and predict[i]==1:\n",
        "      pos = elem[1].find(elem[0])\n",
        "      if pos>=0:\n",
        "        for j in range(pos,pos+len(elem[0])):\n",
        "          spans.append(j)\n",
        "  final_spans.append(spans)\n",
        "\n",
        "print(final_spans[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhOeI2atJuv1"
      },
      "source": [
        "df = pd.read_csv('/content/tsd_trial.csv')\n",
        "\n",
        "\n",
        "from ast import literal_eval\n",
        "df.spans = df.spans.apply(literal_eval)\n",
        "\n",
        "df['predictions'] = pd.Series(final_spans)\n",
        "#df[\"f1_scores\"] = df.apply(lambda row: f1(row.predictions, row.spans), axis=1)\n",
        "df.head()\n",
        "\n",
        "df.to_csv('/content/drive/My Drive/semeval21/res_4929_w2v_dist.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyzzg05wM2cl"
      },
      "source": [
        "f1_scores = []\n",
        "\n",
        "for i in range(690):\n",
        "  f1_scores.append(f1(df['spans'][i],df['predictions'][i]))\n",
        "\n",
        "np.mean(np.array(f1_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyz6JAuaKZW9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}